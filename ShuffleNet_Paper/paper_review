论文《ShuffleNet_ An Extremely Efficient Convolutional Neural Network for Mobile Devices》

1、主要解决的问题：
    在嵌入式设备或移动端设备等计算资源有限的情况下，设计一种能够降低存储和计算代价，但又能保持较高精度的网络。
    ShuffleNet是Face++的一篇关于降低深度网络计算量的论文，号称是可以在移动设备上运行的深度网络。

2、主要亮点：
     "The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy"
     a.pointwise group convolution
     b.channel shuffle

3、逻辑梳理：
    a. 假设input feature map count = N （16）
           output feature map count = M（32），也就是说kernel count=32
        引入group参数，设group=4
        那么，
            input feature map被分成4组，每组4个feature map
            kernel 被分成4组，每组8个kernel
        计算：
            第一组的8个Kernel和第一组的4个feature map 进行卷积得到结果
            第二组的8个Kernel和第二组的4个feature map 进行卷积得到结果
            ......
            得到的结果是：
                g组，M/g个通道的feature map.

        可以对比发现：
            这种分组进行卷积运算将大大降低运算量，因为常规的卷积是kernel的第一个卷积核和每一个通道上对应位置进行卷积并求和，而group convolution将卷积核和feature map进行分组，如上述举例，第一组的8个kernel中的
            每一个，只与第一组的feature map进行卷积计算，分组计算，有效的降低了运算的复杂度。

    b.这样做存在的问题：
        由于将kernel和feature map都进行了分组，那么势必导致边界效应，什么意思呢？就是某个输出channel仅仅来自输入channel的一小部分。这样肯定是不行的的，学出来的特征会非常局限。于是就有了channel shuffle来解决这个问题
    c.解决方法：(channel shuffle 'Channel shuffle.jpg') Channel Shuffle for Group Convolutions
         在进行下一次卷积（GConv2）之前，对其输入feature map做一个分配，也就是每个group分成几个subgroup，然后将不同group的subgroup作为GConv2的一个group的输入，使得GConv2的每一个group都能卷积输入的所有group的feature map，
         这就是channel shuffle的思想。
    d.基本结构 ShuffleUnit（"pointwise group convolutions.jpg"）
        pointwise group convolution ，其实就是含group的卷积核为1*1的pointwise convolution。
        DWConv=depthwise convolution，即深度可分离卷据，
            设input feature map.shape=N*H*W*C
            首先进行深度卷积，设padding='SAME'，那么output feature map.shape=N*H*W*C,即只有卷积核只对每个通道进行卷积而不进行求和。如kernel_size=3
            那么卷积核为3*3*C。
            然后进行1*1*N的逐点卷积，卷积计算和常规卷积一样，对每个通道的卷积进行求和运算。

